# 系统优化和总结

## Hadoop-HA 

所谓HA（High Available），即高可用，实现高可用最关键的策略是消除单点故障。	

![架构](assets/20190402192631.png)

### 基本理论

#### HDFS-HA工作机制

通过双NameNode消除单点故障

#### HDFS-HA工作要点

- Edits日志只有Active状态的NameNode节点可以做写操作
- 两个NameNode都可以读取Edits
- 共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）
- 即同一时刻仅仅有一个NameNode对外提供服务

HA的自动故障转移依赖于ZooKeeper的以下功能：

1. **故障检测**：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。
2. **现役NameNode选择**：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。

### 分布式部署实践

Zookeeper部分请看Zookeeper教程

#### NameNode高可用配置

##### 配置高可用

**（1）core-site.xml**

```xml
<!-- NameNode HA的逻辑访问名称 -->
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://ns1</value>
</property>
<property>
	<name>hadoop.tmp.dir</name>
	<value>/opt/modules/hadoop-2.7.6/data</value>
</property>
```

**（2）hdfs-site.xml**

```xml
<!-- 分布式副本数设置为3 -->
<property>
	<name>dfs.replication</name>
	<value>3</value>
</property>
<!-- 关闭权限检查用户或用户组 -->
<property>
	<name>dfs.permissions.enabled</name>
	<value>false</value>
</property>
<!-- 指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 -->
<property>
	<name>dfs.nameservices</name>
	<value>ns1</value>
</property>
<!-- ns1下面有两个NameNode，分别是nn1，nn2 -->
<property>
	<name>dfs.ha.namenodes.ns1</name>
	<value>nn1,nn2</value>
</property>
<!-- nn1的RPC通信地址 -->
<property>
	<name>dfs.namenode.rpc-address.ns1.nn1</name>
	<value>hadoop01:9000</value>
</property>
<!-- nn1的http通信地址 -->
<property>
	<name>dfs.namenode.http-address.ns1.nn1</name>
	<value>hadoop01:50070</value>
</property>
<!-- nn2的RPC通信地址 -->
<property>
	<name>dfs.namenode.rpc-address.ns1.nn2</name>
	<value>hadoop02:9000</value>
</property>
<!-- nn2的http通信地址 -->
<property>
	<name>dfs.namenode.http-address.ns1.nn2</name>
	<value>hadoop02:50070</value>
</property>
<!-- 指定NameNode的edit文件在哪些JournalNode上存放 -->
<property>
	<name>dfs.namenode.shared.edits.dir</name>
	<value>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/ns1</value>
</property>
<!-- 指定JournalNode在本地磁盘存放数据的位置 -->
<property>
	<name>dfs.journalnode.edits.dir</name>
	<value>/opt/modules/hadoop-2.7.6/journal</value>
</property>
<!-- 当Active出问题后，standby切换成Active，
此时，原Active又没有停止服务，这种情况下会被强制杀死进程。-->
<property>
	<name>dfs.ha.fencing.methods</name>
	<value>
		sshfence
		shell(/bin/true)
	</value>
</property>	
<!-- 使用sshfence隔离机制时需要ssh免登录 -->
<property>
	<name>dfs.ha.fencing.ssh.private-key-files</name>
	<value>/home/fanl/.ssh/id_rsa</value>
</property>
<!-- 配置sshfence隔离机制超时时间 -->
<property>
	<name>dfs.ha.fencing.ssh.connect-timeout</name>
	<value>30000</value>
</property>
```

**分发配置文件到各个服务器**

**（3）启动zookeeper**

```bash
[fanl@hadoop01 zookeeper-3.4.5]$ bin/zkServer.sh start
```

**（4）启动journalnode**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start journalnode
[fanl@hadoop01 hadoop-2.7.6]$ jps
7264 QuorumPeerMain
7346 Jps
7108 JournalNode
```

**（5）格式化NameNode**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ hdfs namenode -format
```

格式化成功后，进行测试

```bash
# a.第一台启动NameNode
[fanl@hadoop01 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start namenode
[fanl@hadoop01 hadoop-2.7.6]$ jps
# b.让另一个namenode 拷贝元数据
[fanl@hadoop02 hadoop-2.7.6]$ bin/hdfs namenode -bootstrapStandby
[fanl@hadoop02 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start namenode
[fanl@hadoop02 hadoop-2.7.6]$ jps
```

![a](assets/20190402192630.png)

```bash
# c.第二个namenode作为active
[fanl@hadoop02 hadoop-2.7.6]$ bin/hdfs haadmin -transitionToActive nn2
```

##### 开启故障自动转移

停止dfs和zookeeper服务

**（1）追加core-site.xml**

```xml
<!-- 向哪个zookeeper注册 -->
<property>
	<name>ha.zookeeper.quorum</name>
	<value>hadoop01:2181,hadoop02.com:2181,hadoop02:2181</value>
</property>
```

**（2）追加 hdfs-site.xml**

```xml
<!--  开启ha自动故障转移 -->
<property>
	<name>dfs.ha.automatic-failover.enabled</name>
	<value>true</value>
</property>		
<property>
	<name>dfs.client.failover.proxy.provider.ns1</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
```

**分发配置文件到各个服务器**

**（3）初始化zkfc**

ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。

```bash
# 启动zookeeper
[fanl@hadoop01 hadoop-2.7.6]$ /opt/modules/zookeeper-3.4.5/bin/zkServer.sh start
[fanl@hadoop01 hadoop-2.7.6]$ bin/hdfs zkfc -formatZK
# =========================
19/04/02 18:56:37 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ns1 in ZK.
```

**（4）启动hdfs**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ sbin/start-dfs.sh
```

![结果](assets/20190402193431.png)

**（5）查看状态**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ hdfs haadmin -getServiceState nn1
[fanl@hadoop01 hadoop-2.7.6]$ hdfs haadmin -getServiceState nn2
```

**（6）模拟故障**

现在nn1 是active，nn2是standby，讲hadoop01上的NameNode进程杀死

```bash
[fanl@hadoop01 hadoop-2.7.6]$ jps
8546 DFSZKFailoverController
8723 Jps
8075 NameNode
7933 QuorumPeerMain
8366 JournalNode
8175 DataNode
[fanl@hadoop01 hadoop-2.7.6]$ kill -9 8075
[fanl@hadoop01 hadoop-2.7.6]$
```

查看控制台，nn2成功变为active，nn1无法访问，重启nn1，变为standby。

模拟成功！

#### YARN-HA

工作机制如下图

![](assets/20190402193432.png)

##### 配置过程

集群规划是，hadoop01和hadoop02部署2个RM

**（1）yarn-site.xml**

```xml
<property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
</property>
<!-- 启用resourcemanager ha -->
<property>
	<name>yarn.resourcemanager.ha.enabled</name>
	<value>true</value>
</property>
<!-- 声明两台resourcemanager的地址 -->
<property>
	<name>yarn.resourcemanager.cluster-id</name>
	<value>cluster-yarn1</value>
</property>
<property>
	<name>yarn.resourcemanager.ha.rm-ids</name>
	<value>rm1,rm2</value>
</property>
<property>
	<name>yarn.resourcemanager.hostname.rm1</name>
	<value>hadoop01</value>
</property>
<property>
	<name>yarn.resourcemanager.hostname.rm2</name>
	<value>hadoop02</value>
</property>
<!-- 指定zookeeper集群的地址 --> 
<property>
	<name>yarn.resourcemanager.zk-address</name>
	<value>hadoop01:2181,hadoop02:2181,hadoop03:2181</value>
</property>
<!-- 启用自动恢复 --> 
<property>
	<name>yarn.resourcemanager.recovery.enabled</name>
	<value>true</value>
</property>
<!-- 指定resourcemanager的状态信息存储在zookeeper集群 --> 
<property>
	<name>yarn.resourcemanager.store.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>
```

**分发该配置文件到其他服务器**

**（2）启动HDFS**

```bash
# =================================#
# 每个部署zookeeper的节点启动zookeeper
[fanl@hadoop01 zookeeper-3.4.5]$ bin/zkServer.sh start
# 在各个JournalNode节点上，输入以下命令启动journalnode服务
[fanl@hadoop01 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start journalnode
[fanl@hadoop01 zookeeper-3.4.5]$ jps
7152 JournalNode
7344 Jps
7304 QuorumPeerMain
# =================================#
# 在[nn1]上，对其进行格式化，并启动
[fanl@hadoop01 hadoop-2.7.6]$ bin/hdfs namenode -format
[fanl@hadoop01 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start namenode
# 在[nn2]上，同步nn1的元数据信息
[fanl@hadoop02 hadoop-2.7.6]$ bin/hdfs namenode -bootstrapStandby
# 启动[nn2]
[fanl@hadoop02 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start namenode
# 启动所有DataNode
[fanl@hadoop01 hadoop-2.7.6]$ sbin/hadoop-daemons.sh start datanode
```

（3）启动YARN

```bash
# 在hadoop01中执行
[fanl@hadoop01 hadoop-2.7.6]$ sbin/start-yarn.sh
[fanl@hadoop01 hadoop-2.7.6]$ jps
7152 JournalNode
7744 DataNode
7304 QuorumPeerMain
7979 NodeManager
7868 ResourceManager
7565 NameNode
8269 Jps
# 在hadoop02中执行
[fanl@hadoop02 hadoop-2.7.6]$ sbin/yarn-daemon.sh start resourcemanager
[fanl@hadoop02 hadoop-2.7.6]$ jps
7157 JournalNode
7718 Jps
7096 QuorumPeerMain
7272 NameNode
7432 DataNode
7675 ResourceManager
7549 NodeManager
# 查看服务状态
[fanl@hadoop01 hadoop-2.7.6]$ bin/yarn rmadmin -getServiceState rm1
active
```



## 相关问题

（1）HDFS HA集群中，jps能看到哪些进程，作用是什么？

