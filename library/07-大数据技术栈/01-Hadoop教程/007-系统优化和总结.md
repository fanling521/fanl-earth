# 系统优化和总结

## Hadoop-HA 

所谓HA（High Available），即高可用，实现高可用最关键的策略是消除单点故障。	

![架构](assets/20190402192631.png)

### 基本理论

#### Hadoop-HA工作机制

通过双NameNode消除单点故障

#### Hadoop-HA工作要点

- Edits日志只有Active状态的NameNode节点可以做写操作
- 两个NameNode都可以读取Edits
- 共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）
- 即同一时刻仅仅有一个NameNode对外提供服务

HA的自动故障转移依赖于ZooKeeper的以下功能：

1. **故障检测**：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。
2. **现役NameNode选择**：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。

### 分布式部署实践

Zookeeper部分请看Zookeeper教程

#### NameNode高可用配置

##### 配置高可用

**（1）core-site.xml**

```xml
<!-- NameNode HA的逻辑访问名称 -->
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://ns1</value>
</property>
<property>
	<name>hadoop.tmp.dir</name>
	<value>/opt/modules/hadoop-2.7.6/data</value>
</property>
```

**（2）hdfs-site.xml**

```xml
<!-- 分布式副本数设置为3 -->
<property>
	<name>dfs.replication</name>
	<value>3</value>
</property>
<!-- 关闭权限检查用户或用户组 -->
<property>
	<name>dfs.permissions.enabled</name>
	<value>false</value>
</property>
<!-- 指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 -->
<property>
	<name>dfs.nameservices</name>
	<value>ns1</value>
</property>
<!-- ns1下面有两个NameNode，分别是nn1，nn2 -->
<property>
	<name>dfs.ha.namenodes.ns1</name>
	<value>nn1,nn2</value>
</property>
<!-- nn1的RPC通信地址 -->
<property>
	<name>dfs.namenode.rpc-address.ns1.nn1</name>
	<value>hadoop01:9000</value>
</property>
<!-- nn1的http通信地址 -->
<property>
	<name>dfs.namenode.http-address.ns1.nn1</name>
	<value>hadoop01:50070</value>
</property>
<!-- nn2的RPC通信地址 -->
<property>
	<name>dfs.namenode.rpc-address.ns1.nn2</name>
	<value>hadoop02:9000</value>
</property>
<!-- nn2的http通信地址 -->
<property>
	<name>dfs.namenode.http-address.ns1.nn2</name>
	<value>hadoop02:50070</value>
</property>
<!-- 指定NameNode的edit文件在哪些JournalNode上存放 -->
<property>
	<name>dfs.namenode.shared.edits.dir</name>
	<value>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/ns1</value>
</property>
<!-- 指定JournalNode在本地磁盘存放数据的位置 -->
<property>
	<name>dfs.journalnode.edits.dir</name>
	<value>/opt/modules/hadoop-2.7.6/journal</value>
</property>
<!-- 当Active出问题后，standby切换成Active，
此时，原Active又没有停止服务，这种情况下会被强制杀死进程。-->
<property>
	<name>dfs.ha.fencing.methods</name>
	<value>
		sshfence
		shell(/bin/true)
	</value>
</property>	
<!-- 使用sshfence隔离机制时需要ssh免登录 -->
<property>
	<name>dfs.ha.fencing.ssh.private-key-files</name>
	<value>/home/fanl/.ssh/id_rsa</value>
</property>
<!-- 配置sshfence隔离机制超时时间 -->
<property>
	<name>dfs.ha.fencing.ssh.connect-timeout</name>
	<value>30000</value>
</property>
```

**分发配置文件到各个服务器**

**（3）启动zookeeper**

```bash
[fanl@hadoop01 zookeeper-3.4.5]$ bin/zkServer.sh start
```

**（4）启动journalnode**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start journalnode
[fanl@hadoop01 hadoop-2.7.6]$ jps
7264 QuorumPeerMain
7346 Jps
7108 JournalNode
```

**（5）格式化NameNode**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ hdfs namenode -format
```

格式化成功后，进行测试

```bash
# a.第一台启动NameNode
[fanl@hadoop01 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start namenode
[fanl@hadoop01 hadoop-2.7.6]$ jps
# b.让另一个namenode 拷贝元数据
[fanl@hadoop02 hadoop-2.7.6]$ bin/hdfs namenode -bootstrapStandby
[fanl@hadoop02 hadoop-2.7.6]$ sbin/hadoop-daemon.sh start namenode
[fanl@hadoop02 hadoop-2.7.6]$ jps
```

![a](assets/20190402192630.png)

```bash
# c.第二个namenode作为active
[fanl@hadoop02 hadoop-2.7.6]$ bin/hdfs haadmin -transitionToActive nn2
```

##### 开启故障自动转移

停止dfs和zookeeper服务

**（1）追加core-site.xml**

```xml
<!-- 向哪个zookeeper注册 -->
<property>
	<name>ha.zookeeper.quorum</name>
	<value>hadoop01:2181,hadoop02.com:2181,hadoop02:2181</value>
</property>
```

**（2）追加 hdfs-site.xml**

```xml
<!--  开启ha自动故障转移 -->
<property>
	<name>dfs.ha.automatic-failover.enabled</name>
	<value>true</value>
</property>		
<property>
	<name>dfs.client.failover.proxy.provider.ns1</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
```

**分发配置文件到各个服务器**

**（3）初始化zkfc**

ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。

```bash
# 启动zookeeper
[fanl@hadoop01 hadoop-2.7.6]$ /opt/modules/zookeeper-3.4.5/bin/zkServer.sh start
[fanl@hadoop01 hadoop-2.7.6]$ bin/hdfs zkfc -formatZK
# =========================
19/04/02 18:56:37 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ns1 in ZK.
```

**（4）启动hdfs**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ sbin/start-dfs.sh
```

![结果](assets/20190402193431.png)

**（5）查看状态**

```bash
[fanl@hadoop01 hadoop-2.7.6]$ hdfs haadmin -getServiceState nn1
[fanl@hadoop01 hadoop-2.7.6]$ hdfs haadmin -getServiceState nn2
```

**（6）模拟故障**

现在nn1 是active，nn2是standby，讲hadoop01上的NameNode进程杀死

```bash
[fanl@hadoop01 hadoop-2.7.6]$ jps
8546 DFSZKFailoverController
8723 Jps
8075 NameNode
7933 QuorumPeerMain
8366 JournalNode
8175 DataNode
[fanl@hadoop01 hadoop-2.7.6]$ kill -9 8075
[fanl@hadoop01 hadoop-2.7.6]$
```

查看控制台，nn2成功变为active，nn1无法访问，重启nn1，变为standby。

模拟成功！

#### YARN高可用配置

